{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gym[atari]\n",
    "!pip install numpy\n",
    "!pip install keras==2.1.2\n",
    "!pip install tensorflow==1.15.5\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPool2D, Flatten, BatchNormalization\n",
    "from keras.optimizers import rmsprop\n",
    "import keras.backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Create The Enviorment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('PongDeterministic-v4')\n",
    "action_space = [4,5] # [\"up\",\"down\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Prepare The Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepro(I):\n",
    "    \"\"\" prepro 210x160x3 uint8 frame into 80x80x1 float \"\"\"\n",
    "    I = I[35:195] # crop\n",
    "    I = I[::2,::2,0] # downsample by factor of 2\n",
    "    I[I == 144] = 0 # erase background (background type 1)\n",
    "    I[I == 109] = 0 # erase background (background type 2)\n",
    "    I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
    "    return I.astype(np.float)[:,:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Create Discounted Reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "def discount_rewards(r):\n",
    "    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(range(len(discounted_r))):\n",
    "        if r[t] != 0: running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
    "        running_add =  r[t] + running_add * gamma # bellman's equation\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r\n",
    "\n",
    "def discount_n_standardise(r):\n",
    "    # standardizes the reward by using z-scores \n",
    "    dr = discount_rewards(r)\n",
    "    dr = (dr - dr.mean()) / dr.std()\n",
    "    return dr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Create The Machine Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(16, kernel_size=(1,1), padding='same', activation='relu', input_shape = (80,80,1)))\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(32, kernel_size=(1,1), padding='same', activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(64, kernel_size=(1,1), padding='same', activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(len(action_space), activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy') #\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "episodes = 0\n",
    "n_episodes = 1000\n",
    "reward_sums = np.zeros(n_episodes)\n",
    "losses = np.zeros(n_episodes)\n",
    "time_steps = np.zeros(n_episodes)\n",
    "reward_sum = 0\n",
    "input_shape = (80,80,1)\n",
    "\n",
    "prev_x = None\n",
    "steps = 2500\n",
    "xs = np.zeros((steps,)+input_shape)\n",
    "ys = np.zeros((steps,1))\n",
    "rs = np.zeros((steps))\n",
    "\n",
    "k = 0\n",
    "observation = env.reset()\n",
    "\n",
    "while episodes < n_episodes:\n",
    "    env.render() # uncomment line to show the gameplay and learning\n",
    "    \n",
    "    # This gets the preprocessed difference of frames to be fed into the model\n",
    "    x = prepro(observation)\n",
    "    xs[k] = x - prev_x if prev_x is not None else np.zeros(input_shape)\n",
    "    prev_x = x\n",
    "    \n",
    "    # This takes an action based on the current model, using random choice to stimuate the enviorment\n",
    "    p = model.predict(xs[k][None,:,:,:])\n",
    "    a = np.random.choice(len(action_space), p=p[0]) #returns either a 0 or a 1\n",
    "    action = action_space[a] #sets a to an action, 0 for up, 1 for down\n",
    "    ys[k] = a #saves the action\n",
    "    \n",
    "    # Renew state of environment\n",
    "    observation, reward, done, info = env.step(action) #takes a step in the enviorment, getting a new set of varibles\n",
    "    reward_sum += reward #record total rewards\n",
    "    rs[k] = reward # record reward per step\n",
    "    \n",
    "    k += 1\n",
    "    \n",
    "    if done or k==steps:\n",
    "        reward_sums[episodes] = reward_sum\n",
    "        reward_sum = 0\n",
    "        \n",
    "        # Gather state, action (y), and rewards (and preprocess)\n",
    "        ep_x = xs[:k]\n",
    "        ep_y = ys[:k]\n",
    "        ep_r = rs[:k]\n",
    "        ep_r = discount_n_standardise(ep_r)\n",
    "        \n",
    "        model.fit(ep_x, ep_y, sample_weight=ep_r, batch_size=512, epochs=1, verbose=0)\n",
    "        \n",
    "        time_steps[episodes] = k\n",
    "        k = 0\n",
    "        prev_x = None\n",
    "        observation = env.reset()\n",
    "        losses[episodes] = model.evaluate(ep_x, \n",
    "                                          ep_y,\n",
    "                                          sample_weight=ep_r,\n",
    "                                          batch_size=512, \n",
    "                                          verbose=0)\n",
    "        \n",
    "        episodes += 1\n",
    "        if np.mean(reward_sums[max(0,episodes-10):episodes]) >= 19:\n",
    "            break\n",
    "\n",
    "            \n",
    "        # Prints out stats such as average loss, average reward, and average steps 20 times over the course of the code\n",
    "        if episodes%(n_episodes//20) == 0:\n",
    "            avg_reward = np.mean(reward_sums[max(0,episodes-200):episodes])\n",
    "            avg_loss = np.mean(losses[max(0,episodes-200):episodes])\n",
    "            avg_time = np.mean(time_steps[max(0,episodes-200):episodes])\n",
    "            plt.plot(reward_sums[:episodes])\n",
    "            plt.show()\n",
    "env.close() # uncomment if env.render() is running"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Plot The Reward Over Episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_from = 0\n",
    "plot_until = 1000\n",
    "plt.plot(reward_sums[plot_from:plot_until])\n",
    "plt.savefig(\"reward_image.jpg\")\n",
    "plt.show()\n",
    "plt.plot(losses[plot_from:plot_until])\n",
    "plt.show()\n",
    "plt.plot(time_steps[plot_from:plot_until])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Watch The Model Play!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = env.reset()\n",
    "prev_frame = None\n",
    "done = False\n",
    "while not done:\n",
    "    x = prepro(observation) \n",
    "    diff = x - prev_frame if prev_frame is not None else np.zeros(input_shape)\n",
    "    p = model.predict(diff[None,:,:,:])\n",
    "    prev_frame = x\n",
    "    a = np.random.choice(len(action_space), p=p[0])\n",
    "    action = action_space[a]\n",
    "    \n",
    "    # Render into buffer. \n",
    "    env.render()\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    time.sleep(.01)\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
